{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mohcine Madkour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collaborator from Dept. Plastic surgery wants to develop an ML-based computational tool, which can tell whether an input ear image is normal or abnormal and, if abnormal, what shape of the ear should be the ideal as results of surgical intervention. The collaborator currently has 300 abnormal and 200 normal ear images. Assuming you are in charge of this project, what would be the best strategy for implementing the tool? Elaborate your strategy, if possible, along with pseudo-codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNN or ConvNet) are by design one of the best models available for most perceptual problems such as image classification, even with very little data to learn from. A CNN convolves learned features with input data, and uses 2D convolutional layers, making this architecture well suited to processing 2D data, such as images.Training a convnet from scratch on a small image dataset will still yield reasonable results, without the need for any custom feature engineering.\n",
    "\n",
    "CNNs eliminate the need for manual feature extraction, so we do not need to identify features used to classify images. The CNN works by extracting features directly from images. The relevant features are not pretrained; they are learned while the network trains on a collection of images. This automated feature extraction makes deep learning models highly accurate for computer vision tasks such as object classification.\n",
    "\n",
    "In CNN the image passes through a series of convolutional, nonlinear, pooling layers and fully connected layers, and then generates the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/cnn1.png \"ShowMyImage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs learn to detect different features of an image using tens or hundreds of hidden layers. Every hidden layer increases the complexity of the learned image features. For example, the first hidden layer could learn how to detect edges, and the last learns how to detect more complex shapes specifically catered to the shape of the object we are trying to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Dataset to Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to split our images to training and validation sets (90%, 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training sets\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "\n",
    "# Split at the given index\n",
    "split_index = int(TRAIN_TEST_SPLIT * n_images)\n",
    "shuffled_indices = np.random.permutation(n_images)\n",
    "train_indices = shuffled_indices[0:split_index]\n",
    "test_indices = shuffled_indices[split_index:]\n",
    "\n",
    "# Split the images and the labels\n",
    "x_train = images[train_indices, :, :]\n",
    "y_train = labels[train_indices]\n",
    "x_test = images[test_indices, :, :]\n",
    "y_test = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/classes.png \"ShowMyImage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because I have a relatively small number of pictures, I augment them via a number of random transformations. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "In Keras this can be done via the *keras.preprocessing.image.ImageDataGenerator* class. This class allows to configure random transformations and normalization operations to be done on an image data during training, and it instantiate generators of augmented image batches (and their labels) via *.flow(data, labels)* or *.flow_from_directory(directory)*. These generators can then be used with the Keras model methods that accept data generators as inputs, *fit_generator*, *evaluate_generator* and *predict_generator*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each single ear shape image, I can generate some pictures and save them to disk, I may want to disable rescaling in this case to keep the images displayable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,`\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('data/train/EARs/ear.0.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .flow() command  generates batches of randomly transformed images and saves the results to the `preview/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction and Training using Convnet from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three most common ways people use deep learning to perform object classification are: training from scratch, transfer learning, and feature extraction. Because I have a new application, I used from scratch method of convnet to train the data, as an initial baseline.\n",
    "\n",
    "In my case I only use a small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation\n",
    "\n",
    "The code snippet below construct the NN model using Keras and TensorFlow, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 150, 150)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of it I stick two fully-connected layers. I end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. To go with it we will also use the binary_crossentropy loss to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use .flow_from_directory() to generate batches of image data (and their labels) directly from the supposedly jpgs in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now use these generators to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feeding images into Convnet and evaluate the model﻿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has finished training, I can evaluate the model on the test set using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on the test set\n",
    "test_predictions = model.predict(x_test)\n",
    "test_predictions = np.round(test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to round the scores so that I get a binary output.\n",
    "\n",
    "Then use the predictions and compare them to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting ideal shapes of abnormal ears after a surgical intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abnormal images represents deformations comparing to normal images, therefore similarity/dissimilarity measurement is quiet difficult. We need to use a suitable AI technique with prior knowledge to get suitable matching score. Otherwise we need to reduce such deformations using possible warping technique as a pre-processing work. \n",
    "\n",
    "To choose a similarity/dissimilarity measurement technique, first we need to know what features of ear biometrie we are trying to compare ? Second, we need to know what we wish to obtain from the comparaison (Do we want to group them? Do we want to simply tell how different are they? Do we want to tell how the images changes with respect to a particular surgurical procedure)?\n",
    "\n",
    "Clearly every different clinical question will have different technical metrics that can quantify image deformation. Correlation analyses can help show the similarity between different images features, however, correlation studies the relationship between one feature and another, not the differences, Bland-Altman analyses provides a method of measuring variability relative to an accepted reference standard. This method is based on the quantification of the agreement between two quantitative measurements by studying the mean difference and constructing limits of agreement.\n",
    "\n",
    "A supervised leanring method for prediction the ideal shape of an image after surgery could be by grouping normal and abnormal shapes into granular groupes (i.e. labeling them into defined classes), and by using the knowledge of a human expert (as we have a limited number of images) and using a ConvNet we can train a CNN model that can predict the class of an input image the same way as we did for classifying normal and abnormal images. As a result the CNN based model can extract the features/values inherent to each class and weight the Neural Network in a way that make it learns this classification.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
